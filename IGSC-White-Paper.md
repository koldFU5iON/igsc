# International Game Studio Classification (IGSC)
## A Bond-Style Rating System for Game Projects

**Version:** 0.4.0
**Author:** Devon Stanton
**Date:** December 2025
**License:** CC BY 4.0

---

## Executive Summary

The video game industry relies on informal labels (Indie, AA, AAA) to classify studios and projects. These terms are culturally defined rather than structurally defined, leading to inconsistency, unfair competition, and a lack of transparency.

The **International Game Studio Classification (IGSC)** proposes a formal, bond-style rating system that rates **projects**, not studios. Drawing inspiration from financial credit ratings, IGSC uses the familiar AAA/AA/A/BBB/BB/B/C scale to indicate production capacity and resource backing. The framework evaluates contributing **sources** independently (studios, publishers, funders) and combines them into a composite project rating.

Rather than a single fuzzy label, projects are rated through:

1. **Source Ratings**: Studios, publishers, and funders are each rated based on their capacity, resources, and track record
2. **Project Rating Combination**: Source ratings are combined using a transparent methodology to produce a project rating
3. **Outcome Tracking**: Post-release performance enables reclassification and lifecycle tracking

The framework is:

- **Voluntary**: projects self-classify through source disclosure
- **Transparent**: all criteria and combination methodology are public
- **Non-invasive**: no sensitive financial disclosure required
- **Project-centric**: same studio can have multiple projects with different ratings simultaneously

This white paper outlines the problem, presents the framework, and invites adoption by platforms, awards bodies, publishers, and the broader industry.

---

## Table of Contents

1. [Introduction](#1-introduction)
2. [The Problem](#2-the-problem)
3. [Why Current Labels Fail](#3-why-current-labels-fail)
4. [The Framework](#4-the-framework)
5. [Classification Dimensions](#5-classification-dimensions)
6. [Tier Definitions](#6-tier-definitions)
7. [How Classification Works](#7-how-classification-works)
8. [Use Cases](#8-use-cases)
9. [Supporting Research & Comparative Analysis](#9-supporting-research--comparative-analysis)
10. [Design Principles](#10-design-principles)
11. [Governance & Evolution](#11-governance--evolution)
12. [Call to Adoption](#12-call-to-adoption)
13. [References](#13-references)

---

## 1. Introduction

The video game industry has grown from a niche hobby into a global entertainment sector generating over $180 billion annually [Newzoo Global Games Marketing Report 2025](https://newzoo.com/resources/trend-reports/newzoo-global-games-market-report-2025). Yet despite this maturation, the industry lacks fundamental infrastructure that other creative sectors take for granted: a standardised way to classify and compare studios.

Film has its budget tiers and production categories. Music has major labels, independent labels, and self-released artists with clear distinctions. Publishing distinguishes between the Big Five, mid-size houses, and small presses. The game industry, by contrast, relies on vague, culturally-defined terms that emerged organically and have never been formalised.

The terms "Indie," "AA," and "AAA" originated in the early 1990's as the industry began distinguishing between different scales of production [wikipedia](https://en.wikipedia.org/wiki/AAA_(video_game_industry). Initially useful shorthand, these labels have become increasingly inadequate as the industry has diversified. A solo developer working from their bedroom, a 30-person studio with venture capital backing, and a 15-person team bootstrapping their first commercial release might all describe themselves as "indie," despite having fundamentally different resources, constraints, and contexts.

This terminology gap creates practical problems across the industry: unfair competition in awards, unclear eligibility for grants, inconsistent media coverage, and difficulty for studios to position themselves accurately. The International Game Studio Classification (IGSC) addresses this gap by proposing a formal, multi-dimensional taxonomy built on verifiable structural criteria rather than subjective cultural labels.

---

## 2. The Problem

The game industry has no standard system for classifying studios or projects. The labels we use, Indie, AA, and AAA, emerged informally and mean different things to different people.

This absence of structure creates real problems:

- **Studios** cannot position themselves accurately or benchmark against true peers
- **Awards and showcases** pit mismatched studios against each other
- **Publishers and investors** lack meaningful segmentation for scouting
- **Grant programmes** struggle to define clear eligibility criteria
- **Press and analysts** report on segments inconsistently
- **Players** have no reliable way to understand a game's production context

The industry deserves better.

---

## 3. Why Current Labels Fail

### 3.1 Definitions Are Inconsistent

There is no universal agreement on what Indie, AA, or AAA mean. Different sources cite different thresholds (Humble Games, 2022; Pearce, cited in Wikipedia):

- "Indie" might mean budget under $1M, team under 10, or simply "no publisher"
- "AA" ranges from "a few million" to "tens of millions" depending on who you ask
- "AAA" is vaguely "big budget, big team, big publisher" without clear boundaries

The same studio could be classified differently depending on which criteria you prioritise.

### 3.2 Labels Conflate Multiple Dimensions

Current classifications muddle together distinct variables:

| Dimension | What It Measures |
|-----------|------------------|
| Budget | How much money is spent |
| Team size | Number of developers |
| Funding source | Self-funded vs publisher-backed |
| Creative independence | Who controls creative decisions |
| Production values | Visual fidelity, scope, polish |
| Distribution | Digital-only vs retail presence |

A single label cannot meaningfully capture all of these. A 40-person studio with publisher funding is fundamentally different from a solo developer, yet both might call themselves "indie."

### 3.3 Wildly Different Studios Get Lumped Together

The current system creates absurd groupings:

- A one-person hobbyist and a 40-person funded studio both labelled "indie"
- A $5M project and a $25M project both called "AA"
- Studios with radically different resources competing in the same category

### 3.4 Unfair Competition

Without clear classification:

- Awards pit mismatched studios against each other
- "Best Indie Game" becomes meaningless when entries range from solo projects to 50-person teams (Conditt, 2023; Wilde, 2023)
- Well-funded studios labelled "indie" crowd out genuine small creators
- Backlash against "fake indies" creates identity friction—as when Dave the Diver's own director stated "there's nothing indie about it" despite its nomination (Hwang, cited in VGC, 2023)

### 3.5 No Lifecycle Tracking

Current labels are static. There is no mechanism to:

- Track how studios grow over time
- Recognise when a studio graduates from one tier to another
- Benchmark success relative to starting context
- Celebrate growth without losing credibility

---

## 4. The Framework

The IGSC addresses these failures through a **source-based project rating system**.

### 4.1 Projects Are Rated, Not Studios

Unlike traditional approaches that classify studios as "indie" or "AAA," IGSC rates **individual projects**. This reflects the reality that studios commit different resources to different projects, and the same studio may simultaneously have projects operating at different scales with different goals.

A studio does not have a single fixed classification. Instead, each project receives a rating based on the sources contributing to it: the studio's capacity, publisher support, funding backing, and other material contributions.

### 4.2 Bond-Style Rating Scale

IGSC adopts the bond rating nomenclature familiar from financial markets: **AAA, AA, A, BBB, BB, B, and C** (with modifiers + and - where applicable).

In financial markets, bond ratings indicate creditworthiness and default risk. Standard & Poor's, Moody's, and Fitch use these scales to communicate the quality and risk profile of debt instruments. AAA represents the highest quality and lowest risk; ratings decline through investment grade (BBB and above) into speculative grade (BB and below).

For game projects, this scale indicates **production capacity and resource backing**, not artistic quality or commercial potential. A AAA-rated project has substantial studio infrastructure, significant publisher support, and major funding. A C-rated project may be a solo developer's passion project with minimal external support. Neither is inherently superior; the rating describes context, not value.

The game industry already uses "AAA" and "AA" informally, but without clear definitions or a complete scale. IGSC formalises this familiar terminology into a structured framework, avoiding the adoption friction that entirely new terminology would face.

### 4.3 Source-Based Rating Methodology

Projects are rated through a three-layer process:

1. **Source Ratings**: Each contributing entity (studio, publisher, funder, platform supporter) is independently rated based on clear criteria
2. **Project Rating Combination**: Source ratings are combined using a transparent weighting methodology to produce a composite project rating
3. **Outcome Classification**: Post-release performance enables reclassification and lifecycle tracking (optional)

This separation enables precise, verifiable assessment. A publisher cannot "make" a project AAA through marketing alone; the studio must have the capacity to execute at that scale. Conversely, a highly capable studio self-publishing will reflect that independence in the rating.

### 4.4 Why This Approach Works

**Prospective Rating**: Projects can be rated before release, during production, or even at concept stage. This enables practical use cases that retrospective systems cannot support: grant eligibility, award category placement, publisher scouting, and studio positioning.

**Transparent and Verifiable**: All source rating criteria are public. Any stakeholder can understand how a project would be rated before participating.

**Context-Aware**: The same studio can have multiple projects with different ratings. A large studio may incubate a small experimental project (rated B or BBB) alongside a flagship title (rated AAA).

**Non-Invasive**: Projects are rated through bracketed ranges and structural indicators, not exact financial disclosure. Studios never need to reveal confidential business information.

### Core Principles

> **Rating must be voluntary, verifiable, and based on proxy metrics rather than confidential financial disclosure.**

Projects should not need to reveal exact budgets or revenue. Rating relies on safe, non-sensitive bands and transparent structural variables.

> **Rating must be prospective, not just retrospective.**

Unlike systems that classify games after release based on output metrics, IGSC rates projects at any stage: before development begins, during production, or after release. This enables grant eligibility determination, award category placement, publisher scouting, and project positioning from day one.

> **Projects are rated individually; studios are sources.**

A studio's capacity contributes to a project's rating, but studios are not themselves rated in aggregate. This reflects the reality of variable resource commitment across projects.

---

## 5. Source Rating Dimensions

Projects are rated by evaluating the contributing sources independently, then combining them. This section outlines the criteria used to rate each source type.

### 5.1 Studio Source Ratings

Studios are rated based on their **capacity to execute** a project, independent of external support. Studio ratings reflect infrastructure, track record, team size, financial stability, and operational maturity.

**Key Variables:**

| Variable | Description | Indicators |
|----------|-------------|------------|
| **Team Size** | Core development headcount dedicated to projects | 1–5 / 6–20 / 21–80 / 80–200 / 200+ |
| **Infrastructure** | Production capabilities, departmental structure, tools/pipelines | Ad-hoc / Emerging / Established / Industry-leading |
| **Track Record** | Shipped commercial titles, proven execution at scale | No releases / First title / Multiple titles / Proven AAA portfolio |
| **Financial Health** | Stability, ownership structure, funding runway | Self-funded / Grant-backed / Investor-backed / Owned by platform/major publisher |
| **Geographic Footprint** | Office presence, distributed capabilities | Home-based / Single office / Multiple offices / International presence |

Studios are rated from **AAA** (200+ staff, proven track record, platform ownership or major publisher backing) down to **C** (solo developers, part-time hobbyist projects). Detailed tier definitions are provided in Section 6.

### 5.2 Publisher/Funder Source Ratings

Publishers and funders are rated based on their **capacity to support and elevate** a project through resources, distribution, and market reach.

**Key Variables:**

| Variable | Description | Indicators |
|----------|-------------|------------|
| **Financial Scale** | Typical budget ranges supported | <$50K / $50K–$250K / $250K–$1M / $1M–$5M / $5M–$30M / $30M+ |
| **Market Reach** | Distribution capabilities, retail presence, marketing | Digital-only / Regional retail / Global distribution / Platform co-marketing |
| **Support Services** | Production support, QA, localisation, live ops, community management | Advisory only / Basic services / Full production support |
| **Platform Relationships** | Preferential access, co-marketing deals, featured placement | Standard terms / Good standing / Preferential treatment |
| **Portfolio Scale** | Concurrent projects, breadth of support | 1–2 projects / 3–10 projects / 10+ concurrent projects |

Publishers/funders are rated from **AAA** (Sony, Microsoft, EA, Take-Two scale: $50M+ budgets, global reach, full services) down to **C** (micro-grants, community support, non-financial backing). Detailed tier definitions are provided in Section 6.

### 5.3 Other Source Types

Beyond studios and publishers, additional sources may contribute to project ratings:

**Platform Holder Support**: ID@Xbox dev kit grants, PlayStation Partners support, Epic MegaGrants. Typically rated **BB-BBB** depending on scale of support.

**Regional/Government Grants**: Film Victoria, Creative Europe, CMF (Canada), UK Games Fund. Typically rated **BBB-A** depending on grant size and support services provided.

**Community Funding**: Kickstarter, Patreon, Fig. Rated **B-BBB** based on amount raised and ongoing community support structure.

**Technology Partners**: Epic (Unreal), Unity, middleware providers offering licensing deals. Treated as modifier influence rather than direct source rating (reduces effective costs, elevates production value potential).

### 5.4 Outcome Metrics (Post-Release)

Outcome metrics reflect performance after a project launches. These are optional and enable reclassification over time, creating a lifecycle view rather than a static rating.

| Variable | Description | Example Bands |
|----------|-------------|---------------|
| **Revenue** | Commercial performance | R0: <$100K / R1: $100K–$1M / R2: $1M–$10M / R3: $10M–$50M / R4: $50M+ |
| **Player Base** | Audience reached | CCU, DAU, or units sold brackets |
| **Critical Reception** | Quality recognition | Review score bands (Metacritic, OpenCritic) |
| **Growth Signals** | Trajectory indicators | Hiring expansion / follow-up funding / sequel greenlit / DLC/live ops launched |

Outcome metrics do not change the initial project rating but may inform re-rating for subsequent projects or sequels from the same sources.

---

## 6. Source Tier Definitions

This section provides detailed definitions for rating studios and publishers/funders using the bond-style scale.

### 6.1 Studio Source Tiers

Studios are rated based on their capacity to execute projects, independent of external support.

#### AAA Studio

**Indicators:**
- **Team size:** 200+ dedicated staff
- **Infrastructure:** Multiple departments (art, design, engineering, audio, QA), established pipelines, proprietary technology or heavily customised engines
- **Track record:** Multiple shipped AAA titles, proven execution at scale
- **Financial health:** Publicly traded OR owned by platform holder or major publisher
- **Geographic footprint:** Multiple offices, international presence

**Examples:** Ubisoft Montreal, EA DICE, Sony Santa Monica, Rockstar North, Bethesda Game Studios

**Modifier guidance:**
- `+` Expanding (hiring wave, new studio openings, parent company investment increase)
- `-` Contracting (layoffs, studio closures, restructuring, reduced parent investment)

---

#### AA Studio

**Indicators:**
- **Team size:** 80–200 staff
- **Infrastructure:** Full departmental structure, established production processes, mature pipelines
- **Track record:** Shipped multiple mid-to-large scope titles
- **Financial health:** Large independent with proven revenue stream OR wholly-owned subsidiary of major publisher
- **Geographic footprint:** Single large office or 2–3 regional offices

**Examples:** Larian Studios (pre-Baldur's Gate 3 scale-up), IO Interactive, Paradox Development Studio, Obsidian Entertainment

**Modifier guidance:**
- `+` Growing (recent commercial hit, active hiring, external validation, new funding round)
- `-` Challenges (project cancellations, funding concerns, key talent departures, revenue difficulties)

---

#### A Studio

**Indicators:**
- **Team size:** 30–80 staff
- **Infrastructure:** Departmental structure emerging, formal processes in place, some specialisation
- **Track record:** 1–2 shipped commercial titles OR strong pedigree from AAA backgrounds
- **Financial health:** Independent with stable revenue stream OR investor-backed with multi-year runway
- **Geographic footprint:** Single office, potentially co-working space or hybrid remote structure

**Examples:** Many "AA game" developers, boutique studios spun out from AAA, successful indie scale-ups entering mid-tier production

**Modifier guidance:**
- `+` Scaling up (active hiring, second project greenlit, series funding secured, publisher deal signed)
- `-` Plateau (difficulty scaling beyond current size, funding runway shortening, hiring challenges)

---

#### BBB Studio

**Indicators:**
- **Team size:** 15–30 staff
- **Infrastructure:** Small departments or defined functional roles, some formal processes
- **Track record:** First commercial title shipped OR experienced leads with unshipped project in development
- **Financial health:** Independent with grants/pre-sales OR seed funding secured
- **Geographic footprint:** Single location, often shared workspace

**Examples:** Studios post-successful Kickstarter campaign, government grant recipients with commercial ambitions, second-time founders with industry experience

**Modifier guidance:**
- `+` External validation (grant awarded, publisher interest expressed, festival selections, community traction)
- `-` Funding uncertainty (runway <12 months, key staff transitioning to part-time, revenue concerns)

---

#### BB Studio

**Indicators:**
- **Team size:** 10–15 staff (mix of full-time and contractors)
- **Infrastructure:** Ad-hoc processes, generalist roles, limited formal structure
- **Track record:** No shipped commercial title OR single small release
- **Financial health:** Self-funded with small grants or pre-sales
- **Geographic footprint:** Remote-first or single small office

**Examples:** First-time studios with experienced individuals, game jam winners scaling up, established hobbyist teams transitioning to commercial

**Modifier guidance:**
- `+` Growing (converting contractors to full-time, grant applications advancing, community funding secured)
- `-` Unstable (team members departing, funding challenges, scope reduction required)

---

#### B Studio

**Indicators:**
- **Team size:** 5–10 people (mostly part-time or contractors)
- **Infrastructure:** Minimal formal process, founder-driven decision-making
- **Track record:** Hobbyist projects or unreleased work only
- **Financial health:** Self-funded from personal savings, nights-and-weekends income
- **Geographic footprint:** Remote or home-based

**Examples:** Passion projects transitioning to commercial ambitions, experienced developers going indie, student/hobbyist teams with commercial aspirations

**Modifier guidance:**
- `+` Transitioning to full-time, community traction growing, crowdfunding campaign successful
- `-` Losing momentum (members leaving, side-project status solidifying, progress slowing)

---

#### CCC/CC/C Studio

**Indicators:**
- **Team size:** 1–4 people
- **Infrastructure:** None, individual workflows
- **Track record:** No commercial releases, learning projects, game jams, portfolio work
- **Financial health:** Fully self-funded, part-time or hobbyist
- **Geographic footprint:** Home-based

**Examples:** Solo developers, small partnerships, most itch.io creators, student projects

**Sub-tier distinctions:**
- **CCC:** 3–4 person team, part-time, first project
- **CC:** 2-person partnership, part-time
- **C:** Solo developer, part-time

**Modifier guidance:**
- `+` Building momentum (Patreon or community support established, planning transition to full-time)
- `-` At risk of abandonment (activity slowing, life circumstances changing, interest waning)

---

### 6.2 Publisher/Funder Source Tiers

Publishers and funders are rated based on their capacity to support and elevate projects through resources, distribution, and market reach.

#### AAA Publisher/Funder

**Indicators:**
- **Financial scale:** $50M+ budgets typical, publicly traded or platform holder
- **Market reach:** Global distribution, retail + digital, major marketing campaigns
- **Support services:** Full production support, QA, localisation, community management, live operations infrastructure
- **Platform relationships:** Preferential treatment, co-marketing deals, featured store placement
- **Portfolio:** Multiple concurrent AAA projects

**Examples:** Sony Interactive Entertainment, Microsoft (Xbox Game Studios), Electronic Arts, Take-Two Interactive, Tencent Games, Activision Blizzard

**Modifier guidance:**
- `+` Increased investment in category (new publishing initiative launched, acquisition spree, expanded signing)
- `-` Pullback (studio closures, reduced signing activity, strategic shift away from category)

---

#### AA Publisher/Funder

**Indicators:**
- **Financial scale:** $5M–$30M budgets, established revenue and market position
- **Market reach:** Strong digital distribution, selective retail presence, moderate marketing budgets
- **Support services:** Production support, QA partnerships, localisation, limited live operations support
- **Platform relationships:** Good standing with platforms, co-marketing on select titles
- **Portfolio:** 3–10 concurrent projects

**Examples:** Devolver Digital, Annapurna Interactive, Private Division, Paradox Interactive (publishing arm), Team17

**Modifier guidance:**
- `+` Growing (recent commercial hits, expanding catalogue, new funding round, increased market visibility)
- `-` Challenges (financial difficulties, key personnel departures, reduced signing pace, portfolio underperformance)

---

#### A Publisher/Funder

**Indicators:**
- **Financial scale:** $1M–$5M budgets
- **Market reach:** Digital-focused, regional retail possible, limited marketing budgets
- **Support services:** Basic production support, QA partnerships, limited localisation
- **Platform relationships:** Standard terms, occasional featured placement
- **Portfolio:** 1–5 concurrent projects

**Examples:** Raw Fury, Fellow Traveller, Humble Games (historically), indie-focused venture capital funds, regional publishers with international ambitions

**Modifier guidance:**
- `+` Expanding (new funding secured, successful releases building reputation, team growth)
- `-` Uncertainty (financial constraints, reduced signing pace, portfolio concentration risk)

---

#### BBB Publisher/Funder

**Indicators:**
- **Financial scale:** $250K–$1M budgets (or equivalent services value)
- **Market reach:** Digital-only, limited marketing support
- **Support services:** Advisory, networking, platform relationship facilitation
- **Portfolio:** 1–3 projects

**Examples:** Regional publishers, indie collectives with funding capacity, platform-specific fund programmes (e.g., ID@Xbox fund grants)

**Modifier guidance:**
- `+` Growing validation (successful releases from portfolio, expanding support capabilities)
- `-` Struggling to provide value (limited resources, portfolio challenges, market position unclear)

---

#### BB Publisher/Funder

**Indicators:**
- **Financial scale:** $50K–$250K (or equivalent services)
- **Market reach:** Distribution support only, minimal marketing
- **Support services:** Advisory and connections primarily
- **Portfolio:** 1–2 projects

**Examples:** Small indie labels, regional grant programmes, angel investors with games industry focus

---

#### B Publisher/Funder

**Indicators:**
- **Financial scale:** <$50K
- **Market reach:** None or minimal
- **Support services:** Advisory only
- **Portfolio:** Single project or advisory role

**Examples:** Micro-grants, mentorship programmes, small angel investors, friends-and-family funding

---

#### CCC/CC/C Publisher/Funder

**Indicators:**
- **Financial scale:** <$10K or non-financial support only
- **Support services:** Minimal to none

**Examples:** Community support, small crowdfunding campaigns, friends and family contributions

---

### 6.3 Combined Project Rating Examples

Projects are rated by combining source ratings. These examples illustrate how different source combinations produce project ratings:

| Studio Rating | Publisher Rating | Other Sources | Likely Project Rating | Example Scenario |
|---------------|------------------|---------------|----------------------|------------------|
| AAA | AAA | Platform co-marketing | **AAA** | First-party AAA title: Sony Santa Monica + SIE publishing God of War |
| AA | AA | Regional grants | **AA** | Mid-tier publisher + established studio: Paradox publishing Obsidian's Pillars of Eternity |
| A | None (self-publishing) | Epic MegaGrant | **A-/BBB+** | Capable studio self-publishing: Supergiant Games' Hades |
| BBB | A | Kickstarter ($500K) | **A-/BBB+** | Small studio with strong publisher: Obsidian + Paradox on Pillars 1 |
| BB | BBB | Government grant | **BBB** | First-time team with regional publisher and grant support |
| B | None | Patreon ($2K/month) | **B+/BB-** | Part-time solo dev with community support: most successful itch.io developers |
| C | None | None | **C** | Solo hobbyist passion project with no external support |

Detailed combination methodology is explained in Section 7.

---

## 7. How Project Rating Works

### 7.1 Rating Process Overview

Projects are rated through a three-step process:

1. **Source Rating**: Each contributing entity (studio, publisher, funder) is independently rated
2. **Combination**: Source ratings are combined using a weighted methodology
3. **Assignment**: Project receives a composite rating reflecting all sources

This process can occur at any stage: concept, pre-production, production, or post-release.

### 7.2 Source Rating

**Studios** provide information about:
- Team size and structure
- Infrastructure and capabilities
- Track record of shipped titles
- Financial health and ownership
- Geographic footprint

**Publishers/Funders** provide information about:
- Financial scale of support
- Market reach and distribution capabilities
- Support services offered (QA, localisation, marketing, etc.)
- Platform relationships
- Portfolio scale

**Other Sources** (grants, platform support, community funding):
- Amount and type of support
- Services or resources provided
- Duration and commitment level

All inputs use bracketed ranges, not exact figures. No confidential financial disclosure is required.

### 7.3 Project Rating Combination Methodology

Once sources are independently rated, they are combined to produce a project rating. IGSC uses a **weighted floor-and-ceiling model** that recognises both studio capacity constraints and external resource elevation.

#### The Recommended Approach: Weighted Floor-and-Ceiling

**Core principle:** Studio capacity sets a practical floor; external resources can elevate but not infinitely.

**Weighting:**
- **Studio source:** 50–60% (foundational capacity)
- **Publisher/primary funder source:** 30–40% (resource elevation)
- **Other sources:** 10–20% (supplementary support)

**Calculation:**
1. Convert each source rating to a numerical score (AAA=95, AA=85, A=75, BBB=65, BB=55, B=45, C=30)
2. Apply weights to each source score
3. Sum to produce a composite score
4. Map back to letter grade with +/- modifiers

**Example:**
- Studio: A (75 points) × 0.55 = 41.25
- Publisher: AA (85 points) × 0.35 = 29.75
- Grant: BBB (65 points) × 0.10 = 6.5
- **Total: 77.5 → Project rating: A**

**Floor constraint:** Project rating cannot exceed studio rating by more than 2 full tiers, even with exceptional external support. A C-rated studio cannot produce a AAA-rated project; infrastructure constraints are real.

**Ceiling constraint:** Project rating cannot be lower than the highest source rating minus 1 tier. If an AAA publisher backs a project, the result cannot fall below AA-, even if the studio is smaller.

#### Why This Approach?

**Considered alternatives:**
- **Pure weighted average**: No constraints, allows unrealistic combinations (C studio + AAA publisher = AA project)
- **Additive tier model**: Less precise, doesn't account for diminishing returns
- **Floor only**: Doesn't recognise significant external elevation (A studio with AAA publisher support deserves AAA rating)

The floor-and-ceiling model balances these concerns, reflecting industry reality: studios need capacity to execute, but strong external support enables projects beyond solo studio capability.

### 7.4 Edge Cases and Modifiers

**Self-publishing:** When no publisher is involved, studio rating heavily influences project rating (70% weight). Other sources (grants, community funding, platform support) fill the remaining 30%.

**Multiple funders:** When multiple publishers or funders support a project, their ratings are averaged before applying the weighting model.

**Modifiers (+/-)**: Applied when sources are at transition points (e.g., studio actively hiring, publisher experiencing growth). Modifiers adjust final rating by approximately half a tier.

**Genre considerations:** Rating reflects resources and capacity, not scope ambition. A narrative walking simulator and an open-world RPG can share a rating if resource backing is equivalent.

### 7.5 Self-Rating Process for Projects

1. **Project lead completes a rating form** with information about:
   - Studio executing the project (team size, infrastructure, track record)
   - Publisher or funder involved (if any)
   - Other sources of support (grants, platform programmes, community funding)
   - Intended platform(s) and scope

2. **System assigns source ratings** based on responses:
   - All inputs use bracketed ranges
   - No sensitive financial disclosure required
   - Process takes approximately 10 minutes

3. **System calculates project rating** using the combination methodology:
   - Weighted scores combined
   - Floor and ceiling constraints applied
   - Modifiers incorporated if applicable

4. **Project receives rating badge/certificate**:
   - Digital badge for press kits, store pages, social media
   - Verification ID for authenticity
   - Listed in public registry (optional)

5. **Re-rating at milestones**:
   - Projects can be re-rated at major milestones (funding rounds, publisher signing, release)
   - Historical ratings preserved for lifecycle tracking
   - Growth and evolution celebrated

### 7.6 What Projects Provide

| Required | Optional |
|----------|----------|
| Studio information (size, structure, track record) | Project website or Steam page |
| Publisher/funder information (if applicable) | Team member LinkedIn profiles |
| Other source information (grants, platform support) | Press kit |
| Intended platform(s) and release window | Development blog or social media |

### 7.7 What Projects Do NOT Provide

- Exact budget figures
- Detailed financial statements
- Confidential business agreements
- Revenue projections or targets
- Investor or funder names (unless already public)

---

## 8. Use Cases

### 8.1 Awards and Showcases

**Problem:** "Best Indie Game" categories pit solo developer passion projects against well-funded 40-person studio productions, creating unfair competition and community backlash.

**Solution:** Create award categories by IGSC project rating tiers:
- Best C/B-Rated Game (solo and small team projects with minimal external support)
- Best BB/BBB-Rated Game (small studios with emerging support structures)
- Best A-Rated Game (mid-tier productions with established backing)
- Best AA/AAA-Rated Game (large-scale productions with substantial resources)

**Benefit:** Fair competition within peer groups based on resource backing, not subjective "indie" definitions. A solo developer rated C competes against other C-rated projects, not against an A-rated 30-person studio with publisher backing.

### 8.2 Publisher and Investor Scouting

**Problem:** Publishers and investors struggle to meaningfully segment the market and discover studios at appropriate scales for their portfolio strategies.

**Solution:** Filter and search projects by IGSC rating and source composition:
- "Show me BBB-rated projects currently seeking AA publisher support"
- "Find A-rated self-published projects that exceeded outcome expectations" (punch-above-weight candidates)
- "Identify B/BB-rated studios with strong track records ready to scale"

**Benefit:** Structured discovery based on transparent criteria. Publishers can target projects matching their support capabilities; investors can identify studios at inflection points.

### 8.3 Grant Programmes and Funding

**Problem:** Government and institutional grant programmes struggle with unclear eligibility criteria, leading to mismatched applicants and difficult adjudication decisions.

**Solution:** Define grant eligibility using IGSC source ratings:
- "This grant is for projects with C/B/BB-rated studios only" (emerging creators)
- "Publisher or funder backing at A-tier or above disqualifies applicants" (ensuring grants reach those who need them)
- "Projects must not exceed BBB rating at time of application" (caps on total resource backing)

**Benefit:** Clear, verifiable eligibility criteria that reduce application burden and adjudication complexity. Grants reach intended recipients without lengthy financial disclosure requirements.

### 8.4 Industry Analysis and Reporting

**Problem:** Industry analysts, researchers, and journalists use inconsistent definitions when segmenting the market, making trend analysis and cross-source comparison impossible.

**Solution:** Standardised IGSC project ratings enable:
- Comparable data across analysts and reports
- Longitudinal trend analysis by rating tier
- Meaningful benchmarking within peer groups
- Clear communication of market segments to stakeholders

**Benefit:** "What percentage of C-rated projects achieve R2+ revenue outcomes?" becomes an answerable research question. Market reports can track how rating tiers correlate with commercial outcomes, development timelines, and platform strategies.

### 8.5 Project Positioning and Communication

**Problem:** Studios struggle to position their projects accurately for press, platforms, and players. "Indie" is both overused and meaningless; "AA" is subjective.

**Solution:** IGSC project ratings provide:
- Honest, verifiable context for project scale and backing
- Clear positioning relative to peer projects
- Track record that can demonstrate growth trajectory
- Credible communication to partners, press, and players

**Benefit:** A studio can say "This is an A-rated project" and stakeholders immediately understand the resource backing and production context, without needing detailed budget disclosure or subjective labelling debates.

### 8.6 Platform Curation and Discovery

**Problem:** Digital storefronts struggle to curate and surface games appropriately. Algorithmic recommendations and editorial features lack context about production scale.

**Solution:** Platforms can integrate IGSC ratings into:
- Curated collections ("Exceptional C-rated games", "Best AA-rated releases this month")
- Discovery algorithms (surface C-rated projects to players interested in small-team innovation)
- Developer programme tiers (offer differentiated support based on project rating)

**Benefit:** Players discover games matching their interests in production context. Platforms can celebrate excellence within peer groups rather than lumping everything into "indie" or "not-indie" buckets.

---

## 9. Supporting Research & Comparative Analysis

This section examines existing approaches to studio and project classification within gaming and adjacent creative industries, providing context for the IGSC framework's design decisions.

### 9.1 Industry Context

The game industry's lack of standardised classification stands in contrast to other creative sectors:

**Film Industry:** Motion pictures are classified by budget tiers with formally defined thresholds through SAG-AFTRA agreements: Ultra Low Budget (under $300K), Moderate Low Budget ($300K–$700K), Low Budget ($700K–$2M), and Basic Theatrical (over $2M) (SAG-AFTRA, 2024). The Motion Picture Association provides content ratings, while production context is communicated through distributor relationships and marketing positioning.

**Music Industry:** The distinction between major labels, independent labels, and self-released artists is well-established. The Association of Independent Music (AIM) formally defines a "major" as a multinational with over 5% world market share, with independents defined as labels not majority-owned by Sony, Warner, or Universal (AIM/IMPALA).

**Publishing Industry:** Book publishing distinguishes between the "Big Five" publishers (Penguin Random House, HarperCollins, Simon & Schuster, Hachette, Macmillan), mid-size houses, small presses, and self-publishing. The Big Five control approximately 80% of the US trade market and generate 64% of industry revenue (WordsRated, 2022).

### 9.2 Existing Game Industry Approaches

Several organisations have attempted partial solutions to classification:

**Platform-Specific Programmes:** Steam, PlayStation, Xbox, and Nintendo each maintain developer programmes, though these are largely unified rather than tiered. Steam Direct charges a flat $100 fee regardless of studio size; ID@Xbox provides two free dev kits to all approved developers; PlayStation Partners and Nintendo Developer Portal operate on case-by-case support allocation rather than formal classification tiers.

**Trade Organisations:** Bodies such as IGDA (International Game Developers Association) and regional equivalents provide community support but have not established formal classification standards. IGDA's Developer Satisfaction Survey defines indie developers simply as "any entity that is independently owned, irrespective of external investments or industry ties" (IGDA DSS).

**Awards Bodies:** The Game Awards, BAFTA Games, and similar organisations have experimented with categories like "Best Independent Game" but apply inconsistent eligibility criteria. The Game Awards publishes no formal definition; BAFTA's rulebook states only that "subsidiaries owned by established studios are not generally eligible... but may be eligible should they be found to be within the spirit of the award" (BAFTA, 2025).

**Market Research Firms:** Newzoo, Sensor Tower, and similar analysts use varying definitions when segmenting the market. Newzoo categorises by price point (indie: ≤$30, AA: $31–50, AAA: $51+), while Gamalytic uses lifetime Steam revenue (indie: $10K–$50M, AA: $50M–$500M, AAA: $500M+)—fundamentally different approaches yielding incompatible classifications.

### 9.3 Academic and Industry Research

Limited formal research exists on game studio classification:

- Academic research has formally examined this problem: Garda and Grabarczyk (2016) distinguish between "indie game" (a cultural/aesthetic category) and "independent game" (meeting criteria of financial, creative, or publishing independence), while Juul's *Handmade Pixels* (MIT Press, 2019) identifies financial, aesthetic, and cultural dimensions of independence
- GDC State of the Industry surveys (conducted annually with 3,000+ respondents, partnering with Omdia for analysis) collect data on studio size but do not propose standardised classification
- Industry commentary frequently highlights the "indie identity" debate without proposing structural solutions—Game Developer's coverage notes "a big discrepancy... between those who think 'independent' means independent funding [versus] independent thought" (Game Developer, 2023)

### 9.4 The HushCrasher Classification System (HCS)

The most significant recent attempt at game classification is the HushCrasher Classification System (HCS 1.0), developed by Antoine Mayerowitz and Julie Belzanne and published in 2024-2025 (Mayerowitz & Belzanne, 2025).

**Methodology:** HCS uses machine learning clustering on Steam data since 2006, enhanced with Mobygames credits data. The system classifies games based on two primary metrics:
- Credits length (number of people credited, excluding QA and special thanks)
- Disk size (game file size in MB/GB)

**Categories:** HCS proposes four tiers: Kei (solo/tiny teams), Midi (small-medium studios), AA, and AAA.

**Key Findings:** HCS analysis revealed that small-scale games ("Kei") now represent 75% of 2024 releases, a 16-fold increase since 2017. Despite this market flooding, these games maintain a stable ~25% revenue share. Median per-game revenue collapsed 97% between 2012-2018.

**Limitations for Project Rating:** While HCS provides valuable market analysis, it addresses a different problem than IGSC:

| Dimension | HCS | IGSC |
|-----------|-----|------|
| **Classifies** | Games (products) | Projects (with source breakdown) |
| **Timing** | Retrospective (post-release only) | Prospective (any development stage) |
| **Primary metrics** | Credits + file size | Studio capacity, publisher/funder resources, support structure |
| **Funding visibility** | None | Source ratings and general scale |
| **Publisher/funder role** | Not considered | Core rating dimension |
| **Self-classification** | No (data-derived) | Yes (voluntary) |
| **Rating nomenclature** | New terminology (Kei, Midi) | Industry-familiar (AAA/AA/A/BBB/BB/B/C) |
| **Bond-style ratings** | No | Yes (borrows from financial credit rating structure) |

HCS cannot rate a project before release, cannot distinguish between a bootstrapped solo developer and one with $2M in venture funding plus AA publisher backing, and introduces new terminology that may face adoption resistance.

**Complementary Approaches:** IGSC and HCS solve different problems and could coexist. IGSC rates the project's resource backing and source structure; HCS classifies the output's production scope based on credits and file size. A BBB-rated project by IGSC might produce a Midi-scale game by HCS metrics, indicating the team punched above their weight in execution.

### 9.5 Why Existing Approaches Fall Short

Current approaches share common limitations:

| Approach | Limitation |
|----------|------------|
| Platform programmes | Proprietary, platform-specific, not transferable |
| Trade organisations | Community-focused, not classification-focused |
| Awards bodies | Ad-hoc definitions, inconsistent year-to-year |
| Market research | Internal methodologies, not publicly standardised |

The IGSC addresses these gaps by providing an open, platform-agnostic, multi-dimensional framework with transparent criteria.

---

## 10. Design Principles

The IGSC is built on these non-negotiable principles:

### 10.1 Voluntary Participation

Rating is opt-in. Projects choose to disclose their sources and receive a rating. No project is rated without consent from the primary stakeholders (studio and publisher/funder, if applicable).

### 10.2 No Sensitive Disclosure

Projects never need to reveal exact budgets, revenue figures, or confidential business agreements. All inputs use safe, bracketed ranges and structural indicators.

### 10.3 Transparent Criteria

The rating methodology is fully public. Any project can understand exactly how it would be rated before participating. Source tier definitions and combination formulas are openly documented.

### 10.4 Neutral Language

Ratings are descriptive, not hierarchical. A C-rated project is not inferior to an AAA-rated project; they represent different resource contexts. The framework avoids value judgements about artistic merit, commercial potential, or quality.

### 10.5 Lifecycle Awareness

Ratings are not permanent. Projects can be re-rated at major milestones (funding rounds, publisher signings, release). Historical ratings are preserved to track evolution and growth over time.

### 10.6 Project-Centric, Not Studio-Centric

Studios are rated as sources, not as entities with fixed classifications. The same studio can have multiple projects with different ratings simultaneously, reflecting variable resource commitment and strategic priorities.

### 10.7 Global Applicability

The framework works across regions. Rating reflects structural capacity and resource backing, not geographic cost differentials. A 30-person studio in Eastern Europe and a 30-person studio in San Francisco are both evaluated on infrastructure, track record, and capabilities, not location-adjusted budgets.

---

## 11. Governance & Evolution

### 11.1 How Standards Gain Authority

Standards are not granted authority; they earn it through adoption. The ESRB (Entertainment Software Rating Board) did not begin with legal authority. It was created by the industry to self-regulate, and became authoritative because retailers and platform holders *chose* to require it. Similarly, PEGI (Pan European Game Information) became the de facto European standard not through legislation, but through voluntary adoption by industry stakeholders who found it useful.

IGSC follows this model. The framework becomes valuable when projects self-rate, when awards bodies use IGSC ratings for categories, when grant programmes reference IGSC for eligibility, and when publishers and platforms filter by IGSC ratings. Each adoption makes the next one easier, creating network effects that compound over time.

### 11.2 Current Governance

The IGSC is maintained by its creator as an open standard. The framework documentation is publicly available, and feedback is welcomed.

### 11.3 Evolution Process

The framework will evolve as the industry changes:

- **Annual review** of tier thresholds, definitions, and combination methodology
- **Community feedback** incorporated into updates
- **Version control** with clear changelogs
- **Backward compatibility** considered for existing project ratings
- **Real-world validation** of combination methodology through industry testing and stakeholder feedback

### 11.4 Future Governance Options

As adoption grows, governance may evolve to include:

- Advisory board with industry representation
- Formal standards body (if warranted)
- Community-driven governance model

The priority is adoption first, formalised governance second.

---

## 12. Call to Adoption

The IGSC is available for adoption by:

### Platforms and Engine Providers
- Integrate IGSC project ratings into game pages and discovery systems
- Use ratings for developer programme segmentation and differentiated support
- Enable analytics and reporting by rating tier
- Surface exceptional projects within peer rating groups

### Awards and Showcases
- Create fair competition categories by IGSC rating tier (C/B, BB/BBB, A, AA/AAA)
- Reduce community backlash about mismatched entries
- Celebrate excellence within peer groups based on resource context

### Publishers and Investors
- Filter and discover projects by rating and source composition
- Benchmark portfolio performance by rating tier
- Identify studios at inflection points (punch-above-weight BBB-rated projects with strong outcomes)
- Make data-driven signing and investment decisions

### Grant Programmes and Funding Bodies
- Define clear, verifiable eligibility criteria using IGSC source ratings
- Cap total project resources to ensure grants reach intended recipients
- Reduce application burden and adjudication complexity
- Track grant impact by rating tier progression

### Press and Analysts
- Use consistent IGSC ratings terminology in coverage and analysis
- Enable meaningful peer-group comparison
- Track industry trends by rating tier distribution and outcomes
- Improve market segmentation in research and reporting

### Studios and Developers
- Self-rate projects to establish transparent positioning
- Benchmark against true peer projects based on resource context
- Track growth trajectory across multiple projects
- Communicate credibly to partners, press, platforms, and players without budget disclosure

---

## 13. References

### Industry Data and Market Reports

- Newzoo. (2024). *Global Games Market Report 2024*. Newzoo B.V. The report found global games market revenues of $187.7 billion in 2024, representing 2.1% year-on-year growth.
- Game Developers Conference & Omdia. (Annual). *GDC State of the Industry Survey*. Survey of 3,000+ game industry professionals conducted October-November each year.

### Comparative Industry Standards

- SAG-AFTRA. (2024). *Low Budget Contracts*. Available at: https://www.sagaftra.org/low-budget
- Association of Independent Music (AIM) / IMPALA. Definition of independent label via ownership threshold (not majority-owned by Sony, Warner, or Universal).
- WordsRated. (2022). "The Big Five Publishers Statistics." Available at: https://wordsrated.com/the-big-five-publishers-statistics/

### Financial Rating Systems

- [CITATION NEEDED: Standard & Poor's, Moody's, or Fitch bond rating methodology documentation explaining AAA/AA/A/BBB/BB/B/C scale used in financial credit ratings]

### Game Industry Classification Attempts

- Mayerowitz, A. & Belzanne, J. (2025). "A Semi-Supervised Taxonomy of Game Production Scope." SSRN. Available at: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5429396
- Mayerowitz, A. & Belzanne, J. (2025). "By the way, what's a AA?" HushCrasher. Available at: https://hushcrasher.substack.com/p/taxonomy-of-games
- HushCrasher Classification System tool. Available at: https://hushcrasher.com/tools/classification/
- The Game Awards. (Annual). Best Independent Game category. No published eligibility definition; nominations by jury of 100+ media outlets.
- BAFTA. (2025). *Games Awards Rulebook*. Available at: https://www.bafta.org/games/awards/eligibility-judging
- Valve Corporation. *Steamworks Partner Program*. Available at: https://partner.steamgames.com/steamdirect
- Microsoft. *ID@Xbox*. Available at: https://www.xbox.com/en-US/developers/id
- Sony Interactive Entertainment. *PlayStation Partners*. Available at: https://partners.playstation.net/
- Nintendo. *Nintendo Developer Portal*. Available at: https://developer.nintendo.com/

### Academic and Industry Commentary

- Garda, M.B. & Grabarczyk, P. (2016). "Is Every Indie Game Independent? Towards the Concept of Independent Game." *Game Studies*, 16(1). Available at: https://gamestudies.org/1601/articles/gardagrabarczyk
- Juul, J. (2019). *Handmade Pixels: Independent Video Games and the Quest for Authenticity*. MIT Press.
- Conditt, J. (2023). "The Game Awards raises an old question: What does indie mean?" *Engadget*. Available at: https://www.engadget.com/the-game-awards-raises-an-old-question-what-does-indie-mean-205211035.html
- Wilde, T. (2023). "Should facts or vibes determine whether an 'indie game' is really indie?" *PC Gamer*. Available at: https://www.pcgamer.com/indie-game-debate-dave-the-diver/
- Lennox, J. (2023). "The Game Award's indie problem isn't just a linguistic debate." *Digital Trends*. Available at: https://www.digitaltrends.com/gaming/indie-games-game-awards-2023-debate/
- Orland, K. (2024). "Here Come the AAAA Games... But What's a AAA Game and Why Do We Call Them That?" *Video Game Canon*. Available at: https://www.videogamecanon.com/adventurelog/what-is-a-aaa-game/ (citing Video Game History Foundation Digital Archive, documenting first use of "AAA" in *Electronic Gaming Retail News*, September 1991)

### Trade Organisation Resources

- IGDA. (Annual). *Developer Satisfaction Survey*. Available at: https://igda.org/dss/
- Humble Games. (2022). "Identifying Indie: A Study of Who Plays What and Why." *GDC Vault*. Available at: https://gdcvault.com/play/1028798/Identifying-Indie-A-Study-of

---

## License

This work is licensed under [Creative Commons Attribution 4.0 International (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/).

You are free to:
- **Share**: copy and redistribute in any medium or format
- **Adapt**: remix, transform, and build upon the material for any purpose, including commercially

Under the following terms:
- **Attribution**: You must give appropriate credit to IGSC and Devon Stanton

---

## Contact

For questions, feedback, or adoption enquiries, please open an issue on this repository or contact the author.

**Devon Stanton**
Creator, International Game Studio Classification (IGSC)

---

*International Game Studio Classification (IGSC): Bringing clarity to an industry that deserves better.*
